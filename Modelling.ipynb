{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/liuying1201/Credit-Risk-Prediction-German-Bank/blob/main/Modelling.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "deTWHVrtORe-"
      },
      "source": [
        "# Machine learning german bank credit risk prediction\n",
        "In this project, I will build a <b>machine learning model to predict credit risk of German Bank's customers</b>. It involves <b>supervised learning (using a labeled training set) for classification</b>, where the <b>target</b> is <b>1</b> if the <b>customer</b> represents a <b>bad risk</b>, and <b>0</b> if he represents a <b>good risk</b>.\n",
        "\n",
        "I will use the following <b>pipeline:</b>\n",
        "\n",
        "<b>1. Define the business problem.</b><br>\n",
        "<b>2. Collect the data and get a general overview of it.</b><br>\n",
        "<b>3. Split the data into train and test sets.</b><br>\n",
        "<b>4. Explore the data (exploratory data analysis)</b><br>\n",
        "<b>5. Data cleaning and preprocessing.</b><br>\n",
        "<b>6. Model training, comparison, selection and tuning.</b><br>\n",
        "<b>7. Final production model testing and evaluation.</b><br>\n",
        "<b>8. Conclude and interpret the model results.</b><br>\n",
        "<b>9. Deploy.</b><br>\n",
        "\n",
        "In <b>this notebook</b>, I will perform <b>machine learning modelling, covering steps 5 to 8 of the pipeline above</b>. The main <b>objective</b> here is to <b>build a model that can predict as many as possible bad risk customers, thus recall is the metric of interest</b>. Once I build this model, the <B>bank can make informed decisions that balance profit generation with prudent risk management</b>, ultimately benefiting both the company and its customers. Furthermore, I will approach these steps in more detail below, with an overview of them and the explanation of why I am making each decision."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ifsiNLD-ORfB"
      },
      "source": [
        "### Importing the libraries"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 395
        },
        "id": "iwhrqmM2ORfC",
        "outputId": "d51ad576-09d2-4e0f-cec8-1b472004b181"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "ModuleNotFoundError",
          "evalue": "No module named 'category_encoders'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-1-3918fc1c5671>\u001b[0m in \u001b[0;36m<cell line: 12>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompose\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mColumnTransformer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpreprocessing\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mOrdinalEncoder\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mStandardScaler\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mcategory_encoders\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mTargetEncoder\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     13\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimpute\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mSimpleImputer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmetrics\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mclassification_report\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mroc_auc_score\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconfusion_matrix\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mroc_curve\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'category_encoders'",
            "",
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0;32m\nNOTE: If your import is failing due to a missing package, you can\nmanually install dependencies using either !pip or !apt.\n\nTo view examples of installing some common dependencies, click the\n\"Open Examples\" button below.\n\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n"
          ],
          "errorDetails": {
            "actions": [
              {
                "action": "open_url",
                "actionText": "Open Examples",
                "url": "/notebooks/snippets/importing_libraries.ipynb"
              }
            ]
          }
        }
      ],
      "source": [
        "# Data manipulation and visualization.\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "# Modelling\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.compose import ColumnTransformer\n",
        "from sklearn.preprocessing import OrdinalEncoder, StandardScaler\n",
        "from category_encoders import TargetEncoder\n",
        "from sklearn.impute import SimpleImputer\n",
        "from sklearn.metrics import classification_report, roc_auc_score, confusion_matrix, roc_curve\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.svm import LinearSVC, SVC\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from xgboost import XGBClassifier\n",
        "from skopt import BayesSearchCV\n",
        "from skopt.space import Real, Categorical, Integer\n",
        "from sklearn.base import clone\n",
        "\n",
        "# Fixing bayesian search error.\n",
        "np.int = int\n",
        "\n",
        "# Filter warnings.\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "# Utils\n",
        "from notebooks.modelling_utils import *"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5cHWEFXAORfE"
      },
      "source": [
        "### Reading the dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1H2gOOzSORfE"
      },
      "outputs": [],
      "source": [
        "df = pd.read_csv('data/german_credit_data.csv')\n",
        "# Dropping irrelevant column.\n",
        "df.drop(columns=['Unnamed: 0'], inplace=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "s4kQrFgjORfF"
      },
      "outputs": [],
      "source": [
        "df.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Rm4mTgHeORfF"
      },
      "source": [
        "Sex and Risk can be expressed as binary features. Thus, I will set good as 1 and bad as 0 in Risk. In sex, I will set male as 1 and female as 0."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uHBrEso0ORfG"
      },
      "outputs": [],
      "source": [
        "df['Sex'] = df['Sex'].map({'male': 1, 'female': 0})\n",
        "df['Risk'] = df['Risk'].map({'bad': 1, 'good': 0})"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qCK-MsT_ORfG"
      },
      "source": [
        "### Split the data into train and test sets\n",
        "- First of all, I will split the data into train and test sets.\n",
        "- Test set is supposed to be data the model has never seen before.\n",
        "- This split will avoid data leakage, which occurs when information from the test set or future data inadvertently leaks into the training process, leading to over-optimistic performance estimate, compromising the model's ability to generalize to new, unseen data.\n",
        "- A good practice to protect the model against data leakage is perform fit_transform on training data and just transform on test data when applying preprocessing steps after the split.\n",
        "- I will specify stratify=y so that the train_test_split function ensures that the splitting process maintains the same percentage of each target class in both the training and testing sets. This is particularly useful when dealing with imbalanced datasets, which is the case, as there are more good risk customers."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nbpE9pzgORfG"
      },
      "outputs": [],
      "source": [
        "X = df.drop(columns=['Risk'])\n",
        "y = df['Risk'].copy()\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, stratify=y, random_state=42)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "I-wfBs0LORfH"
      },
      "outputs": [],
      "source": [
        "print(f'Train predictor dataset shape: {X_train.shape}.')\n",
        "print(f'Train target dataset shape: {y_train.shape}.')\n",
        "print(f'Test predictor dataset shape: {X_test.shape}.')\n",
        "print(f'Test target dataset shape: {y_test.shape}.')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ACbX0eOwORfH"
      },
      "outputs": [],
      "source": [
        "print(f'Train target classes proportions: ')\n",
        "print(y_train.value_counts(normalize=True))\n",
        "print(f'\\nTest target classes proportions:')\n",
        "print(y_test.value_counts(normalize=True))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "z4b4Kew8ORfH"
      },
      "source": [
        "The train-test-split was succesfull and each target class proportion was preserved in both sets."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "F491E1jMORfH"
      },
      "source": [
        "# 5. Data cleaning and preprocessing\n",
        "In order to fit machine learning algorithms, it is necessary to apply some transformations to the data.\n",
        "\n",
        "- <b>Impute missing values:</b> There are some missing values in Saving accounts and Checking account. I will replace them with mode. Dropping the null values is not a good choice because I have a little amount of data (just 1000 rows).<br>\n",
        "- <b>Outliers:</b> In the eda step, I investigated the outliers and saw that they don't represent inconsistent information like measurement errors. Thus, considering this and the fact that I have a little amount of data, I will not drop or treat any.<br>\n",
        "- In order to train tree-based algorithms, it is not necessary to scale the data. However, as I intend to test a different algorithms, I will apply standard scaler on numerical and categorical features (after encoding). By doing this, it will be possible to compare all models at once.\n",
        "- <b>Numerical features:</b> Some algorithms are sensitive to feature scaling because they use distance calculations or optimizations like gradient descent for being trained, which are affected by scale. Thus, I will apply StandardScaler on numeric attributes.<br>\n",
        "- <b>Categorical features:</b> Machine learning algorithms make mathematical calculations, so it's necessary to convert the categoric attributes to numeric by applying encoding techniques. Saving accounts and Checking account present ordinal relationships. Thus, I chose OrdinalEncoder for encoding them. Moreover, I chose TargetEncoder for the other categorical features because OneHotEncoder would increase dimensionality (creating dummy variables). As I have a little amount of data, increase dimensionality would favor overfitting, compromising my model's generalization ability. After all these variables are encoded/converted to numeric, I will apply StandardScaler because some algorithms are sensitive to scale. StandardScaler is more robust to outliers than MinMaxScaler, that's why I chose it.<br>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "E_GKC5CDORfI"
      },
      "outputs": [],
      "source": [
        "X_train.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WA3W30GzORfI"
      },
      "source": [
        "Checking missing values."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2ymEbj6DORfI"
      },
      "outputs": [],
      "source": [
        "X_train.isna().sum()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IEDB8ZFoORfI"
      },
      "source": [
        "Checking numerical features."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nJKkQiIVORfI"
      },
      "outputs": [],
      "source": [
        "numerical_features = X_train.select_dtypes('number').columns.to_list()\n",
        "print(f'There are {len(numerical_features)} numerical features. They are: {numerical_features}')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tvifO4qEORfJ"
      },
      "source": [
        "Checking categorical features cardinality and unique values."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Jyak6H7pORfJ"
      },
      "outputs": [],
      "source": [
        "categorical_features = X_train.select_dtypes('object').columns.to_list()\n",
        "\n",
        "print(f'There are {len(categorical_features)} categorical features. They are: {categorical_features}')\n",
        "print()\n",
        "\n",
        "for feature in categorical_features:\n",
        "    print(feature)\n",
        "    print(f'Number of categories: {X_train[feature].nunique()}. They are:')\n",
        "    print(X_train[feature].unique())\n",
        "    print()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4s2yRWZfORfJ"
      },
      "source": [
        "I will build the preprocessor below. As I intend to test different algorithms, the same preprocessor will be used for tree-based models and scale sensitive models. Moreover, I will apply the techniques that I've mentioned above."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ags-mIRkORfK"
      },
      "outputs": [],
      "source": [
        "target_encoder_features = ['Saving accounts', 'Checking account']\n",
        "ordinal_encoder_features = ['Purpose', 'Housing']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "f8LdratuORfK"
      },
      "outputs": [],
      "source": [
        "ordinal_pipeline = Pipeline(\n",
        "    steps=[\n",
        "        ('imputer', SimpleImputer(strategy='most_frequent')),\n",
        "        ('ordinal_encoder', OrdinalEncoder()),\n",
        "        ('std_scaler', StandardScaler())\n",
        "        ]\n",
        "        )\n",
        "\n",
        "target_pipeline = Pipeline(\n",
        "    steps=[\n",
        "        ('target_encoder',TargetEncoder(cols=target_encoder_features)),\n",
        "        ('std_scaler', StandardScaler())\n",
        "        ]\n",
        "        )\n",
        "\n",
        "preprocessor = ColumnTransformer(\n",
        "    transformers=[\n",
        "        ('ordinal', ordinal_pipeline, ordinal_encoder_features),\n",
        "        ('target', target_pipeline, target_encoder_features),\n",
        "        ('std_scaler', StandardScaler(), numerical_features)\n",
        "        ], remainder='passthrough'\n",
        "        )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "m2Yc03Z9ORfK"
      },
      "outputs": [],
      "source": [
        "X_train_prepared = preprocessor.fit_transform(X_train, y_train)\n",
        "X_train_prepared.shape"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iI5VQKKyORfL"
      },
      "source": [
        "# 6. Model training, comparison, selection and tuning\n",
        "- In this step, I intend to compare different models performances. In order to achieve this, I will use stratified k-fold cross validation to train each one of them and evaluate their ROC-AUC score. Accuracy is not a good metric because the target is imbalanced. Moreover, stratified k-fold cross validation will maintain the target proportion on each fold, dealing with the imbalanced target as well.\n",
        "- <b>K-fold cross-validation</b> is a technique used in machine learning to assess the performance of a model. It involves dividing the dataset into K subsets, using K-1 for training and one for testing iteratively. This helps in estimating a model's generalization ability by reducing the risk of overfitting and providing more reliable performance metrics.<br>\n",
        "- My objective here is to select a best model to go for hyperparameter tuning. In order to get this best model, I will evaluate the highest roc-auc average validation scores and look at the bias-variance trade-off.\n",
        "- After selecting a model, I will tune its hyperparameters.\n",
        "- <b>Hyperparameter tuning</b> refers to the process of selecting the optimal hyperparameters for a machine learning model. Hyperparameters are parameters that are set before the model is trained and directly influence its performance but are not learned from the data."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "o-P1JsEcORfL"
      },
      "source": [
        "Training the models and comparing their performances (roc-auc score) with stratified k-fold cross validation in order to choose one of them for hyperparameter tuning."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CfgajHuxORfL"
      },
      "outputs": [],
      "source": [
        "models = {\n",
        "    'Logistic Regression': LogisticRegression(),\n",
        "    'Linear SVM': LinearSVC(),\n",
        "    'SVM': SVC(),\n",
        "    'K-Nearest Neighbors': KNeighborsClassifier(),\n",
        "    'Random Forest': RandomForestClassifier(),\n",
        "    'XGBoost': XGBClassifier()\n",
        "}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Pa2TC7kfORfL"
      },
      "outputs": [],
      "source": [
        "eval_df = evaluate_models_cv(models=models, X_train=X_train_prepared, y_train=y_train)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4SNuz_zsORfM"
      },
      "outputs": [],
      "source": [
        "eval_df.sort_values(['Average Val Score'], ascending=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8IUWz6IQORfN"
      },
      "source": [
        "Logistic Regression presents the highest average validation ROC-AUC score. Furthermore, Random Forest is overfitting. There is a significant difference between its validation and training errors. In fact, the training error is zero. Thus, the Random Forest model exhibits low bias and high variance, and since its mean validation score is only slightly lower than that of Logistic Regression, I will choose it for hyperparameter tuning and final model evaluation because there is more room for performance improvement (due to the overfit)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LIPGskBxORfN"
      },
      "source": [
        "### Hyperparameter tuning\n",
        "As I intend to tune Random Forest model, I won't use GridSearchCV.\n",
        "\n",
        "<b>Why not use grid search?</b>\n",
        "- Using grid search CV to tune Random Forest can be problematic due to its computational expense, limitations in granularity and adaptability, lack of consideration for interaction effects, inefficient exploration of the hyperparameter space, and inefficient allocation of computational resources.\n",
        "- Grid search exhaustively searches through all combinations of specified hyperparameters, it does not adapt its search based on the observed performance of previous hyperparameter configurations. Also, hyperparameters in Random Forest can have complex interactions, where the impact of one hyperparameter depends on the values of others. Grid search does not explicitly consider these interactions and evaluates hyperparameters independently.\n",
        "Considering this, I will tune Random Forest using Bayesian Optimization.\n",
        "\n",
        "<b>What is Bayesian Optimization?</b>\n",
        "-  Bayesian optimization is an efficient and adaptive technique for finding the optimal combination of hyperparameters for a machine learning model. It uses probabilistic models to intelligently explore the hyperparameter space, balancing exploration and exploitation."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dcUkU29hORfN"
      },
      "source": [
        "An important point here is to define class_weight hyperparameter. Then, the algorithm will assign different weights for the target's minority (bad risk) and majority (good risk) class instances. Thus, the model will be penalized when mistakenly predicting a bad risk customer and also be able to better learn the patterns in minority class data samples."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NFrD6xyYORfO"
      },
      "outputs": [],
      "source": [
        "n_folds = 5\n",
        "stratified_kfold = StratifiedKFold(n_splits=n_folds, shuffle=True, random_state=42)\n",
        "\n",
        "search_space = {\n",
        "    'n_estimators': Integer(100, 700),\n",
        "    'criterion': Categorical(['gini', 'entropy', 'log_loss']),\n",
        "    'max_depth': Integer(2, 16),\n",
        "    'min_samples_split': Integer(2, 50),\n",
        "    'min_samples_leaf': Integer(2, 25),\n",
        "    'bootstrap': Categorical([True, False]),\n",
        "    'class_weight': Categorical(['balanced', 'balanced_subsample']),\n",
        "    'max_features': Categorical(['sqrt', 'log2', None])\n",
        "}\n",
        "\n",
        "bayesian_search = BayesSearchCV(estimator=RandomForestClassifier(), search_spaces=search_space, cv=stratified_kfold, n_iter=50, scoring='roc_auc', return_train_score=True, random_state=42)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "suU8sVwYORfO"
      },
      "outputs": [],
      "source": [
        "bayesian_search.fit(X_train_prepared, y_train)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dSPpXkMpORfO"
      },
      "outputs": [],
      "source": [
        "print(f'The best params fund for Random Forest are: ')\n",
        "bayesian_search.best_params_"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LS6z36lhORfO"
      },
      "source": [
        "Let's take a look into some results obtained by the bayesian search cv, specially for important parameters like class_weight, criterion, max_depth, min_samples_leaf, min_samples_split and n_estimators."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ol4YFdlnORfP"
      },
      "outputs": [],
      "source": [
        "bayesian_search_results = pd.DataFrame(bayesian_search.cv_results_)\n",
        "bayesian_search_results = bayesian_search_results[['rank_test_score', 'mean_test_score',\n",
        "'mean_train_score', 'param_class_weight',\n",
        "'param_criterion', 'param_max_depth',\n",
        "'param_min_samples_leaf', 'param_min_samples_split',\n",
        "'param_n_estimators']]\n",
        "bayesian_search_results.sort_values(['rank_test_score']).head(10)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CT7HyB7FORfP"
      },
      "source": [
        "The best model was reached applying regularization! Clearly, bias has increased (higher training error) and thus variance has decreased (lower average validation error), improving the model's generalization ability. It is possible to see that higher values were defined for parameters like min_samples_leaf and min_samples_split and lower values were defined for parameters like max_depth."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bNx5_hpDORfP"
      },
      "source": [
        "# 7. Final production model testing and evaluation\n",
        "- In this step, I intend to evaluate our final tuned model on test data (simulating production unseen data).\n",
        "- Once I am facing a binary classification problem with an imbalanced target, recall is the most important metric. I want my model to identify as many as possible bad risk customers. Therefore, we can be more flexible with lower precision values, as false positives (customers who are good risk but were classified as bad risk) will not cause many issues for the bank.\n",
        "- Moreover, I will look at precision-recall trade-off to see wheter there is space for recall metric improvement without significantly compromising the precision score. This is a good strategy when dealing with imbalanced data.\n",
        "- By identifying a large portion of the bad risk customers, the bank can make informed decisions, balancing profit generation and prudent risk management and minimizing potential financial losses caused when a borrower fail to pay his credit obligations. This is the main objective of the project."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZYSSv0ENORfP"
      },
      "source": [
        "Getting the final tuned Random Forest model."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QiYOZfXkORfP"
      },
      "outputs": [],
      "source": [
        "final_rf_clf = bayesian_search.best_estimator_"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uENI8plhORfQ"
      },
      "source": [
        "Using the preprocessor to transform the test predictor set. By applying just transform we avoid data leakage."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jWfgCp5LORfQ"
      },
      "outputs": [],
      "source": [
        "X_test_prepared = preprocessor.transform(X_test)\n",
        "X_test_prepared.shape"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "m-4EsT_IORfQ"
      },
      "source": [
        "Making predictions."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8sRQaHh9ORfQ"
      },
      "outputs": [],
      "source": [
        "final_predictions = final_rf_clf.predict(X_test_prepared)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "i0GP5pYEORfQ"
      },
      "source": [
        "### Evaluating the final tuned Random Forest model."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "r1Z5Iff8ORfQ"
      },
      "outputs": [],
      "source": [
        "evaluate_classifier(y_test, final_predictions)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ed7ahet_ORfQ"
      },
      "source": [
        "<b>The model presents a decent performance!</b>\n",
        "\n",
        "<b>Recall Score (0.68):</b> The recall score (metric of interest for this analysis), also known as the true positive rate or sensitivity, measures the proportion of actual positive cases (bad risk customers) that the model correctly identifies. The model correctly identifies 68% of the bad risk customers. In practical terms, looking at the confusion matrix, this means that it has defined correctly 41 out of 60 bad risk customers.\n",
        "\n",
        "<b>Precision Score (0.49):</b> The precision score quantifies the proportion of correctly predicted positive cases (bad risk customers) out of all instances predicted as positive by the model. Out of all the customers predicted as bad risk, 49% of them actually belong to this class. In practical terms, looking at the confusion matrix, this means that from 83 clients predicted as bad risk, 41 of them are actually risky. Thus, the precision score is not so good. However, predicting good risk customers as bad risk (false positives) will not cause many issues for the bank. Our goal is really to correctly identify a significant portion of bad risk customers, and the model does it.\n",
        "\n",
        "<b>AUC Score (0.69):</b> The AUC (Area Under the Curve) score represents the overall performance of the model in distinguishing between positive (bad risk) and negative (good risk) instances. It measures the probability that the model will rank a randomly chosen positive instance higher than a randomly chosen negative instance. A higher AUC score indicates better performance. With an AUC score of 0.69, the model demonstrates a good ability to differentiate between good and bad risk customers."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VXSoeCoXORfR"
      },
      "source": [
        "### Interpreting the model results with feature importances\n",
        "I will look at <b>feature importances</b> below, in order to interpret Random Forest results. An important task when performing supervised learning on a dataset is determining which features provide the most predictive power. By focusing on the relationship between only a few crucial features and the target label we simplify our understanding of the phenomenon, which is most always a useful thing to do.\n",
        "\n",
        "Random Forest calculates feature importances by aggregating the average decrease in impurity (Gini or entropy) across all decision trees in the ensemble. Features that lead to more significant impurity reduction when used for splitting are considered more important. The final importance scores are normalized to sum up to 1 across all features."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7nDERE6YORfR"
      },
      "outputs": [],
      "source": [
        "plot_feature_importances(final_rf_clf, df)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pITJ3a0aORfR"
      },
      "source": [
        "Housing, Purpose, Duration, Job and Saving accounts are the 5 features with the highest predictive power. I saw on the eda that customers within some Housing, Purpose, Job and Saving accounts categories like free, vacation/others, highly skilled and little presented a tendency of being any of the credit risk labels (bad or good). Furthermore, I also saw that longer durations credit services are associated with higher credit amounts and thus, higher levels of risk.\n",
        "\n",
        "Finally, considering this, the model results are consistent and interpretable. Everything makes sense."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ET8iB3KNORfR"
      },
      "source": [
        "### Precision vs Recall trade-off\n",
        "- My objective in building the model is to correctly predict as many bad risk customers as possible, such that the bank can make informed decisions and minimize risk when offering its credit services. Thus, I will examine the precision-recall curve to determine whether there is room for enhancing the recall metric without significantly compromising the precision score. This is a typical procedure when handling an imbalanced target in binary classification. Finally, I will do the predictions comparing the estimated probabilities of being bad risk (positive) with the threshold that provides the target recall value. If the estimated probability is greater than the threshold, the instance will be classified as positive, else negative.\n",
        "- <b>Precision-recall trade-off:</b> The precision-recall trade-off involves finding a balance between precision and recall in a classification model. For Random Forest, adjusting the decision threshold can impact this trade-off. Lowering the threshold increases recall but may decrease precision, while raising it has the opposite effect. This trade-off is crucial for optimizing model performance, especially when dealing with imbalanced datasets."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "czAukA7mORfS"
      },
      "source": [
        "Getting the model's estimated probabilities."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "B5jGc038ORfS"
      },
      "outputs": [],
      "source": [
        "predicted_probas = final_rf_clf.predict_proba(X_test_prepared)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6cCXi3VRORfS"
      },
      "source": [
        "Plotting precision recall curve for different threshold values."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SXxvnvqSORfS"
      },
      "outputs": [],
      "source": [
        "precision, recall, threshold = precision_vs_recall_curve(y_test, predicted_probas)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5hhytEFcORfT"
      },
      "source": [
        "There is space for enhancing the recall metric without significantly compromising the precision score."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bmk3PAhGORfT"
      },
      "source": [
        "Getting the threshold for a 0.8 recall score."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lPiSneoWORfT"
      },
      "outputs": [],
      "source": [
        "threshold_precision, threshold_recall, selected_threshold = get_threshold_metrics(precision=precision,\n",
        "recall=recall, threshold=threshold,\n",
        "target_metric='recall', target_metric_value=0.8)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Rbfqty9BORfT"
      },
      "source": [
        "Making predictions."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "exkOOyGgORfT"
      },
      "outputs": [],
      "source": [
        "threshold_predictions = (predicted_probas[:, 1] >= selected_threshold).astype(int)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zqhGHAWkORfU"
      },
      "source": [
        "Evaluating final model after balancing the precision-recall trade-off."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Gi2Uwg3RORfU"
      },
      "outputs": [],
      "source": [
        "evaluate_classifier(y_test, threshold_predictions)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3VcoQ7ziORfU"
      },
      "source": [
        "The results are great! By dealing with precision-recall trade-off, I was able to select a threshold value that improved the recall metric from 0.68 to 0.8 without compromising the precision score, which fell from 0.49 to 0.48, just a little downgrade! In practical terms, looking at the confusion matrix, it was possible to correctly predict 48 out of 60 bad risk customers. At the same time, I could also improve the ROC-AUC score from 0.69 to 0.71. Finally, the objective was achieved. Now, the bank can correctly identify 80% of the bad risk customers. Thus, my model solves the business problem by allowing the bank to make informed decisions, balancing profit generation with prudent risk management."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-b4dmKW3ORfU"
      },
      "source": [
        "### Visualizing the probabilities\n",
        "Below I get the top 10 customers with respect to highest and lowest Random Forest model's estimated probabilities of being bad risk. As we can see, the model results are coherent! Sometimes it makes errors, but mostly it is correct with respect to risky clients."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WaTZD6WfORfU"
      },
      "source": [
        "Top 10 customers with highest model's estimated probabilities of being bad risk."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "H1X7STVSORfU"
      },
      "outputs": [],
      "source": [
        "bad_risk_probas = predicted_probas[:, 1]\n",
        "\n",
        "probas_df = pd.concat([X_test, y_test], axis=1)\n",
        "probas_df['Bad Risk Probas'] = bad_risk_probas\n",
        "probas_df.sort_values(['Bad Risk Probas'], ascending=False).head(10)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RfQMIGBYORfU"
      },
      "source": [
        "Top 10 customers with lowest model's estimated probabilities of being bad risk."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LdGqQ6toORfV"
      },
      "outputs": [],
      "source": [
        "probas_df.sort_values(['Bad Risk Probas'], ascending=True).head(10)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CF-VlNBOORfV"
      },
      "source": [
        "The model's estimated probabilities make sense!\n",
        "- Looking at the two customers with highest probabilities of being bad risk, they don't have a house, present little Saving account and borrowed a high credit amount for a long duration. Moreover, both of them did it for business purposes, configuring higher levels of risk! Business is about risk taking and, since they have a low balance saving account, they were correctly classified as bad risk with some certainty.\n",
        "- Looking at the customers with lowest model's estimated probabilities of being bad risk, it is clear that, having a house, taking lower credit amounts for shorter durations for car or radio/tv purposes can configure a client as good risk.\n",
        "- The insights found in the EDA step and feature importances were confirmed by the model's estimated probabilities.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IpbGBiWmORfV"
      },
      "source": [
        "# 8. Conclusions\n",
        "- In this project, I have built a Random Forest Classifier model for predicting credit risk of German Bank's customers. The objective of the project was to build a model that was able to correctly identify as many as possible bad risk customers, focusing on the recall score. By doing this, the bank can make informed decisions, balancing profit generation with prudent risk management and minimizing potential financial losses caused when a borrower fails to repay a loan or a credit obligation.\n",
        "- The business problem is solved now. My model is able to correctly predict 80% of the bad risk customers! Furthermore, the eda step provided a lot of useful insights that helped us understand the problem and credit risk patterns within the available features.\n",
        "- The main techniques used to deal with the imbalanced target were stratified split, stratified k-fold cross validation, class weight Random Forest model hyperparameter and precision-recall trade-off balancing. I didn't intend to use SMOTE, because it is prone to overfitting and a lot of experient data scientists don't recommend it.\n",
        "- First, I have reached a 0.68 recall score by applying hyperparameter tuning on Random Forest. Then, I looked at the precision-recall trade-off, finding a threshold that provided a recall of 0.8. I made the predictions again, now comparing the model's estimated probabilities of the customer being bad risk with this threshold. If the probability was greater than it, the instance was classified as positive, else negative. By doing this, the sensitivity in fact went from 0.68 to 0.8 without compromising the precision metric, which fell from 0.49 to 0.48.\n",
        "- The model results were interpreted and they are coherent! By looking at feature importances, it was possible to assess the features with the highest predictive power for the analysis. In the EDA step, I had already thought these features would be the most useful, reinforcing that the estimator outcomes make sense. Finally, the model's estimated probabilities also make sense and thus everything is working fine.\n",
        "- The next step is to deploy the model using CI/CD pipeline principles, with modular coding."
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.4"
    },
    "orig_nbformat": 4,
    "colab": {
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}